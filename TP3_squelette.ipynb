{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# École Polytechnique de Montréal\n",
    "\n",
    "Département Génie Informatique et Génie Logiciel\n",
    "INF8460 – Traitement automatique de la langue naturelle\n",
    "\n",
    "#### Prof. Amal Zouaq\n",
    "#### Chargé de laboratoire: Félix Martel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3\n",
    "\n",
    "## Objectifs\n",
    "\n",
    " - Implanter des modèles de classification neuronaux\n",
    " - Utiliser des plongements lexicaux pré-entrainés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire\n",
    "\n",
    "Comme dans le TP précédent, on va travailler sur l'analyse de sentiment en utilisant les données du [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Vous devez utiliser `scikit-learn` pour la régression logistique, et Keras pour les modèles neuronaux.\n",
    "\n",
    "Les sections 1, 2 et 3 sont indépendantes.\n",
    "\n",
    "Vous êtes libres d'appliquez les pré-traitements que vous jugerez utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pré-traitement et lecture de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est un test.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('test.txt', 'r') as mon_fichier:\n",
    "    texte = mon_fichier.read()\n",
    "\n",
    "print(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, vous devez lire vos données et appliquez les pré-traitements que vous jugerez utiles\n",
    "\n",
    "# On fait comme dans le TP 2\n",
    "import os\n",
    "\n",
    "seq_train_neg = os.listdir(\"aclImdb/train/neg\")\n",
    "seq_train_pos = os.listdir(\"aclImdb/train/pos\")\n",
    "\n",
    "data_train_neg = [open(\"aclImdb/train/neg/\" + file,  encoding=\"utf-8\").read() for file in seq_train_neg]\n",
    "data_train_pos = [open(\"aclImdb/train/pos/\" + file,  encoding=\"utf-8\").read() for file in seq_train_pos]\n",
    "data_train = data_train_pos + data_train_neg\n",
    "\n",
    "\n",
    "\n",
    "seq_test_neg = os.listdir(\"aclImdb/test/neg\")\n",
    "seq_test_pos = os.listdir(\"aclImdb/test/pos\")\n",
    "\n",
    "data_test_neg = [open(\"aclImdb/test/neg/\" + file,  encoding=\"utf-8\").read() for file in seq_test_neg]\n",
    "data_test_pos = [open(\"aclImdb/test/pos/\" + file,  encoding=\"utf-8\").read() for file in seq_test_pos]\n",
    "data_test = data_test_pos + data_test_neg\n",
    "\n",
    "nb_doc_train = len(data_train)\n",
    "nb_doc_test = len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "data_train_neg = data_train_neg[:1000]\n",
    "data_train_pos = data_train_pos[:1000]\n",
    "data_train = data_train_pos + data_train_neg\n",
    "nb_doc_train = len(data_train)\n",
    "nb_doc_test = len(data_test)\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\mametc/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mametc\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\mametc/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mametc\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5ea6b7346cb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\mametc/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mametc\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def segmentize(raw_text):\n",
    "    \"\"\"\n",
    "    Segmente un document en phrases.\n",
    "\n",
    "    >>> raw_corpus = \"Alice est là . Bob est ici\"\n",
    "    >>> segmentize(raw_text)\n",
    "    [\"Alice est là .\", \"Bob est ici\"]\n",
    "\n",
    "    :param raw_text: str\n",
    "    :return: list(str)\n",
    "    \"\"\"\n",
    "    return nltk.sent_tokenize(raw_text)\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize une liste de phrases en mots.\n",
    "\n",
    "    >>> sentences = [\"Alice est là \", \"Bob est ici\"]\n",
    "    >>> corpus = tokenize(sentences)\n",
    "    >>> corpus_name\n",
    "    [\n",
    "        [\"Alice\", \"est\", \"là \"],\n",
    "        [\"Bob\", \"est\", \"ici\"]\n",
    "    ]\n",
    "\n",
    "    :param sentences: list(str), une liste de phrases\n",
    "    :return: list(list(str)), une liste de phrases tokenizées\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        res.append(nltk.word_tokenize(sentence))\n",
    "    return res\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    \"\"\"\n",
    "    Remove the stopwords defined in nltk.corpus.stopwords from the given tokenized text.\n",
    "    Nous devons enlever certains stopwords à la main.\n",
    "    \n",
    "    :param tokenized_text: list(list(str)), une liste de listes de tokens\n",
    "    :return: list(list(str)), une liste de listes de tokens, sans les tokens correspondant aux stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    res = list()\n",
    "    for sentence in tokenized_text:\n",
    "        sentence_mod = list()\n",
    "        for token in sentence:\n",
    "             if token not in stopwords and token != \"br\" and token != \"The\" and token != \"This\":\n",
    "                sentence_mod.append(token)\n",
    "        res.append(sentence_mod)\n",
    "    return res\n",
    "\n",
    "def keep_only_alphabetic_characters(tokenized_text):\n",
    "    \"\"\"\n",
    "    Remove non-alphabetic characters from the text\n",
    "    \n",
    "    :param tokenized_text: list(list(str)), une liste de liste de tokens\n",
    "    :return: list(list(str)), une liste de listes de tokens sans caractère non-alphabétiques\n",
    "    \"\"\"\n",
    "\n",
    "    res = list()\n",
    "    for sentence in tokenized_text:\n",
    "        sentence_mod = list()\n",
    "        for token in sentence:\n",
    "             if re.fullmatch(\"[a-zA-Z]*\",token)!=None:\n",
    "                sentence_mod.append(token)\n",
    "        res.append(sentence_mod)\n",
    "    return res\n",
    "\n",
    "def remove_lone_characters(tokenized_text):\n",
    "    \"\"\"\n",
    "    Remove one-character words from the text.\n",
    "    \n",
    "    :param tokenized_text: list(list(str)), une liste de liste de tokens\n",
    "    :return: list(list(str)), une liste de listes de tokens sans caractères seuls\n",
    "    \"\"\"\n",
    "    res = list()\n",
    "    for sentence in tokenized_text:\n",
    "        sentence_mod = list()\n",
    "        for token in sentence:\n",
    "             if len(token) > 1:\n",
    "                sentence_mod.append(token)\n",
    "        if sentence_mod != []:\n",
    "            res.append(sentence_mod)\n",
    "    return res\n",
    "\n",
    "def clean_doc(corpus):\n",
    "    tokens_preprocessed = []\n",
    "    for documents in corpus:\n",
    "        sentences = segmentize(documents)\n",
    "        tokens = tokenize(sentences)\n",
    "        tokens_stopword_free = remove_stopwords(tokens)\n",
    "        tokens_alphabetic = keep_only_alphabetic_characters(tokens_stopword_free)\n",
    "        tokens_preprocessed.append(remove_lone_characters(tokens_alphabetic))\n",
    "    return (tokens_preprocessed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite, vous allez entraîner deux modèles neuronaux : un perceptron multi-couche (MLP pour *multi-layer perceptron*), un réseau récurrent LSTM bi-directionnel. Pour cela, vous devrez utiliser la librairie [Keras](https://keras.io/).\n",
    "\n",
    "N'hésitez pas à expérimenter différents hyper-paramètres pour obtenir le meilleur résultat possible sur au moins un de vos réseaux(nombre de couches, dimension des couches, etc.). Quelques pistes:\n",
    "\n",
    "- optimisation des hyper-paramètres avec validation croisée (tous modèles)\n",
    "\n",
    "- réduction de la dimension avec une LSA (MLP)\n",
    "\n",
    "- ajout de couches/augmentation de la dimension/dropout ou autres changements d'architecture (MLP ou LSTM)\n",
    "\n",
    "- pré-traitement différent (tous modèles)\n",
    "\n",
    "\n",
    "<mark>Il est **fortement conseillé** d'utiliser une machine avec GPU</mark> pour entraîner ces modèles neuronaux. Vous pouvez utiliser les machines du labo L-4818 ou faire tourner votre notebook sur [Google Colab](https://colab.research.google.com) (gratuit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-layer Perceptron\n",
    "\n",
    "**a)** Ici, on vous demande d'entraîner un perceptron multi-couches sur la matrice TF-IDF. Avant l'entraînement, affichez la structure du modèle avec `model.summary()`. Précisez la structure du réseau de neurones (taille et nombre de couches) et les paramètres d'entraînement (optimiseur, nombre d'époques, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bi-directional LSTM\n",
    "\n",
    "**b)** Toujours avec Keras, entraînez un modèle bi-LSTM sur le corpus d'entraînement. Comme précédemment, affichez la structure du réseau et indiquez les paramètres utiles.\n",
    "\n",
    "*Note :* si votre machine supporte CUDA, vous pouvez utiliser `keras.layers.CuDNNLSTM` au lieu de `keras.layers.LSTM` pour des gains de performance significatifs. Sur Google Colab, les environnements avec GPU supportent CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-26404f036877>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-26404f036877>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    Dense(1, activation=\"softmax\")])\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "n_features = 10\n",
    "sequence_len = 50\n",
    "lstm_model = Sequential([\n",
    "                        LSTM(64, input_shape=(sequence_len, n_features),\n",
    "                        Dense(1, activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensoflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement tensoflow (from versions: )\n",
      "No matching distribution found for tensoflow\n",
      "You are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: h5py in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: pyyaml in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras) (3.12)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras) (1.1.0)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "Could not install packages due to an EnvironmentError: [WinError 5] Accès refusé: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\Lib\\\\site-packages\\\\keras_preprocessing'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "You are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensoflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = {r\"i'm\": 'i am',\n",
    "                r\"'re\": ' are',\n",
    "                r\"let’s\": 'let us',\n",
    "                r\"'s\":  ' is',\n",
    "                r\"'ve\": ' have',\n",
    "                r\"can't\": 'can not',\n",
    "                r\"cannot\": 'can not',\n",
    "                r\"shan’t\": 'shall not',\n",
    "                r\"n't\": ' not',\n",
    "                r\"'d\": ' would',\n",
    "                r\"'ll\": ' will',\n",
    "                r\"'scuse\": 'excuse',\n",
    "                ',': ' ,',\n",
    "                '.': ' .',\n",
    "                '!': ' !',\n",
    "                '?': ' ?',\n",
    "                '\\s+': ' '}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    for s in replace_list:\n",
    "        text = text.replace(s, replace_list[s])\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(map(lambda p: clean_text(p), data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life , such as \"teachers\" . my 35 years in the teaching profession lead me to believe that bromwell high is satire is much closer to reality than is \"teachers\" . the scramble to survive financially , the insightful students who can see right through their pathetic teachers' pomp , the pettiness of the whole situation , all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school , i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line: inspector: i am here to sack one of your teachers . student: welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it is not !\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max phrase len: 1978\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phrase_len = list(map(lambda p: len(p.split(' ')), X_train))\n",
    "max_phrase_len = max(phrase_len)\n",
    "print('max phrase len: {0}'.format(max_phrase_len))\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.hist(phrase_len, alpha = 0.2, density = True)\n",
    "plt.xlabel('phrase len')\n",
    "plt.ylabel('probability')\n",
    "plt.grid(alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [1 for i in range(len(data_train_pos))] + [0 for i in range(len(data_train_neg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 8192\n",
    "tokenizer = Tokenizer(\n",
    "    num_words = max_words,\n",
    "    filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~'\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen = max_phrase_len)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1978 8192\n"
     ]
    }
   ],
   "source": [
    "print(max_phrase_len, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 64)                523008    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 523,073\n",
      "Trainable params: 523,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(64, input_shape = (len(data_train), max_phrase_len)))\n",
    "lstm.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "lstm.summary()\n",
    "\n",
    "lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_11_input to have 3 dimensions, but got array with shape (2000, 1978)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d5fe7fd3a222>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_11_input to have 3 dimensions, but got array with shape (2000, 1978)"
     ]
    }
   ],
   "source": [
    "history = lstm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.1,\n",
    "    epochs = 8,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, dropout=0.3, recurrent_dropout=0.3, input_shape=(1978, 819..., units=256)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c5232586782e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#model_lstm.add(Embedding(input_dim = max_words, output_dim = 256, input_length = max_phrase_len))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#model_lstm.add(SpatialDropout1D(0.3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_phrase_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#model_lstm.add(Dropout(0.3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'units'"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "#model_lstm.add(Embedding(input_dim = max_words, output_dim = 256, input_length = max_phrase_len))\n",
    "#model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(256, activation = 'relu'))\n",
    "#model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(2, activation = 'softmax'))\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-38bc9dc4c8e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;31m# to match the value shapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "history = model_lstm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.1,\n",
    "    epochs = 8,\n",
    "    batch_size = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Word Embeddings**\n",
    "\n",
    "Pour améliorer le modèle précédent, on va utiliser des *word embeddings* (ou plongements vectoriels) pré-entraînés. \n",
    "\n",
    "On utilisera un modèle Skip-gram de dimension $d=300$ entraîné sur English Wikipedia, disponible à l'adresse [http://vectors.nlpl.eu/explore/embeddings/en/models/](http://vectors.nlpl.eu/explore/embeddings/en/models/). Dans cette archive, vous trouverez les embeddings dans un fichier `.txt` tel que \n",
    "- la première ligne du fichier indique le nombre de mots dans le vocabulaire et la dimension des embeddings\n",
    "- chacune des lignes suivantes est composée d'un mot_étiquette grammaticale suivi des 300 coordonnées de son *embedding*, le tout séparé par des espaces. \n",
    "\n",
    "Ainsi, les premières lignes de ce fichier sont :\n",
    "```\n",
    "296630 300\n",
    "also_ADV -0.010121 -0.045202 -0.065609 ... -0.065423\n",
    "one_NUM -0.060427 0.005032 -0.076370 ... -0.107769\n",
    "first_ADJ 0.005799 0.024848 0.018902 ...  -0.097193\n",
    "...\n",
    "```\n",
    "\n",
    "Les étiquettes `_ADV`, `_NUM`, `_ADJ`, etc. indiquent l'étiquette grammaticale du mot et peuvent être supprimées pour ce TP.\n",
    "\n",
    "*Note :* vous pouvez utiliser le snippet suivant pour télécharger et dézipper automatiquement les embeddings (pratique si vous utilisez une machine distante comme Google Colab) :\n",
    "```python\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "\n",
    "res = requests.get(\"http://link/to/archive.zip\")\n",
    "with ZipFile(io.BytesIO(res.content)) as z:\n",
    "  z.extractall(\"extract/to/dir/\")\n",
    "```\n",
    "\n",
    "\n",
    "Implémentez un modèle bi-LSTM qui utilisent ces *embeddings* pour représenter les mots d'une phrase. Vous pourrez utiliser le layer [Embedding](https://keras.io/layers/embeddings/) de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Indiquez les performances de chacun de vos modèles. Comparez avec les modèles Naive Bayes et character-LM du TP précédent et commentez.\n",
    "\n",
    "Quel est votre meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
