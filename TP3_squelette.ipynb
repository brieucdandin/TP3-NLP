{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# École Polytechnique de Montréal\n",
    "\n",
    "Département Génie Informatique et Génie Logiciel\n",
    "INF8460 – Traitement automatique de la langue naturelle\n",
    "\n",
    "#### Prof. Amal Zouaq\n",
    "#### Chargé de laboratoire: Félix Martel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3\n",
    "\n",
    "## Objectifs\n",
    "\n",
    " - Implanter des modèles de classification neuronaux\n",
    " - Utiliser des plongements lexicaux pré-entrainés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire\n",
    "\n",
    "Comme dans le TP précédent, on va travailler sur l'analyse de sentiment en utilisant les données du [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Vous devez utiliser `scikit-learn` pour la régression logistique, et Keras pour les modèles neuronaux.\n",
    "\n",
    "Les sections 1, 2 et 3 sont indépendantes.\n",
    "\n",
    "Vous êtes libres d'appliquez les pré-traitements que vous jugerez utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pré-traitement et lecture de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, vous devez lire vos données et appliquez les pré-traitements que vous jugerez utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite, vous allez entraîner deux modèles neuronaux : un perceptron multi-couche (MLP pour *multi-layer perceptron*), un réseau récurrent LSTM bi-directionnel. Pour cela, vous devrez utiliser la librairie [Keras](https://keras.io/).\n",
    "\n",
    "N'hésitez pas à expérimenter différents hyper-paramètres pour obtenir le meilleur résultat possible sur au moins un de vos réseaux(nombre de couches, dimension des couches, etc.). Quelques pistes:\n",
    "\n",
    "- optimisation des hyper-paramètres avec validation croisée (tous modèles)\n",
    "\n",
    "- réduction de la dimension avec une LSA (MLP)\n",
    "\n",
    "- ajout de couches/augmentation de la dimension/dropout ou autres changements d'architecture (MLP ou LSTM)\n",
    "\n",
    "- pré-traitement différent (tous modèles)\n",
    "\n",
    "\n",
    "<mark>Il est **fortement conseillé** d'utiliser une machine avec GPU</mark> pour entraîner ces modèles neuronaux. Vous pouvez utiliser les machines du labo L-4818 ou faire tourner votre notebook sur [Google Colab](https://colab.research.google.com) (gratuit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-layer Perceptron\n",
    "\n",
    "**a)** Ici, on vous demande d'entraîner un perceptron multi-couches sur la matrice TF-IDF. Avant l'entraînement, affichez la structure du modèle avec `model.summary()`. Précisez la structure du réseau de neurones (taille et nombre de couches) et les paramètres d'entraînement (optimiseur, nombre d'époques, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bi-directional LSTM\n",
    "\n",
    "**b)** Toujours avec Keras, entraînez un modèle bi-LSTM sur le corpus d'entraînement. Comme précédemment, affichez la structure du réseau et indiquez les paramètres utiles.\n",
    "\n",
    "*Note :* si votre machine supporte CUDA, vous pouvez utiliser `keras.layers.CuDNNLSTM` au lieu de `keras.layers.LSTM` pour des gains de performance significatifs. Sur Google Colab, les environnements avec GPU supportent CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Word Embeddings**\n",
    "\n",
    "Pour améliorer le modèle précédent, on va utiliser des *word embeddings* (ou plongements vectoriels) pré-entraînés. \n",
    "\n",
    "On utilisera un modèle Skip-gram de dimension $d=300$ entraîné sur English Wikipedia, disponible à l'adresse [http://vectors.nlpl.eu/explore/embeddings/en/models/](http://vectors.nlpl.eu/explore/embeddings/en/models/). Dans cette archive, vous trouverez les embeddings dans un fichier `.txt` tel que \n",
    "- la première ligne du fichier indique le nombre de mots dans le vocabulaire et la dimension des embeddings\n",
    "- chacune des lignes suivantes est composée d'un mot_étiquette grammaticale suivi des 300 coordonnées de son *embedding*, le tout séparé par des espaces. \n",
    "\n",
    "Ainsi, les premières lignes de ce fichier sont :\n",
    "```\n",
    "296630 300\n",
    "also_ADV -0.010121 -0.045202 -0.065609 ... -0.065423\n",
    "one_NUM -0.060427 0.005032 -0.076370 ... -0.107769\n",
    "first_ADJ 0.005799 0.024848 0.018902 ...  -0.097193\n",
    "...\n",
    "```\n",
    "\n",
    "Les étiquettes `_ADV`, `_NUM`, `_ADJ`, etc. indiquent l'étiquette grammaticale du mot et peuvent être supprimées pour ce TP.\n",
    "\n",
    "*Note :* vous pouvez utiliser le snippet suivant pour télécharger et dézipper automatiquement les embeddings (pratique si vous utilisez une machine distante comme Google Colab) :\n",
    "```python\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "\n",
    "res = requests.get(\"http://link/to/archive.zip\")\n",
    "with ZipFile(io.BytesIO(res.content)) as z:\n",
    "  z.extractall(\"extract/to/dir/\")\n",
    "```\n",
    "\n",
    "\n",
    "Implémentez un modèle bi-LSTM qui utilisent ces *embeddings* pour représenter les mots d'une phrase. Vous pourrez utiliser le layer [Embedding](https://keras.io/layers/embeddings/) de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Indiquez les performances de chacun de vos modèles. Comparez avec les modèles Naive Bayes et character-LM du TP précédent et commentez.\n",
    "\n",
    "Quel est votre meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
